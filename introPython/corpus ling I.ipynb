{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\husem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\husem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\husem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\husem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\husem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall\tNN\n",
      "I\tPRP\n",
      "compare\tVBP\n",
      "data\tNNS\n",
      "Yes\tUH\n",
      "We\tPRP\n",
      "use\tVBP\n",
      "complicated\tVBN\n",
      "methods\tNNS\n",
      "compare\tVBP\n",
      "do\tVBP\n",
      "shake\tVB\n",
      "darling\tVBG\n",
      "hot\tVBZ\n",
      "is\tVBZ\n",
      "changing\tVBG\n",
      "fade\tVB\n",
      "lose\tVBP\n",
      "breathe\tVB\n",
      "see\tVB\n",
      "gives\tVBZ\n",
      "thee\tVB\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'macbeth.xml'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 122>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcodecs\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01metree\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mElementTree\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mElementTree\u001B[39;00m\n\u001B[0;32m    122\u001B[0m tei_header, tei_text \u001B[38;5;241m=\u001B[39m ElementTree\u001B[38;5;241m.\u001B[39mfromstring(\n\u001B[1;32m--> 123\u001B[0m     \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmacbeth.xml\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m    124\u001B[0m )\n\u001B[0;32m    125\u001B[0m textnodes_act1 \u001B[38;5;241m=\u001B[39m [textnodes \u001B[38;5;28;01mfor\u001B[39;00m textnodes \u001B[38;5;129;01min\u001B[39;00m tei_text[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitertext()]\n\u001B[0;32m    126\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(textnodes_act1)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Python39\\lib\\codecs.py:905\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(filename, mode, encoding, errors, buffering)\u001B[0m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    902\u001B[0m    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m    903\u001B[0m     \u001B[38;5;66;03m# Force opening of the file in binary mode\u001B[39;00m\n\u001B[0;32m    904\u001B[0m     mode \u001B[38;5;241m=\u001B[39m mode \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 905\u001B[0m file \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffering\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    907\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'macbeth.xml'"
     ]
    }
   ],
   "source": [
    "# conda install -n dphil nltk textblob\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")  # Punkt sentence tokenizer\n",
    "nltk.download(\"brown\")  # Brown reference corpus\n",
    "nltk.download(\"wordnet\")  # Lemmatization using wordnet\n",
    "nltk.download(\"omw-1.4\")  # Open Multilingual Wordnet\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # POS tagger\n",
    "# https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"\"\"\n",
    "Shall I compare thee to a summer’s day?\n",
    "Thou art more lovely and more temperate;\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer’s lease hath all too short a date;\n",
    "\n",
    "Sometime too hot the eye of heaven shines,\n",
    "And often is his gold complexion dimm’d;\n",
    "And every fair from fair sometime declines,\n",
    "By chance or nature’s changing course untrimm’d;\n",
    "\n",
    "But thy eternal summer shall not fade,\n",
    "Nor lose possession of that fair thou ow’st;\n",
    "Nor shall Death brag thou wander’st in his shade,\n",
    "When in eternal lines to time thou grow’st:\n",
    "\n",
    "So long as men can breathe or eyes can see,\n",
    "So long lives this, and this gives life to thee.\n",
    "\"\"\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "len(blob)\n",
    "\n",
    "blob[0]\n",
    "\n",
    "blob[1:16]\n",
    "\n",
    "# sentence segementation might not be suitable for lyrics\n",
    "len(blob.sentences)\n",
    "\n",
    "blob.sentences[0]\n",
    "\n",
    "# sentences seem to only stop at \"?\" and \".\"\n",
    "[string[-1] for string in blob.sentences]\n",
    "\n",
    "\n",
    "newblob = blob[1:16] + \" \" + blob[168:171] + \"a? Yes! We use complicated methods.\"\n",
    "\n",
    "len(newblob.sentences)\n",
    "\n",
    "newblob.sentences[0]\n",
    "newblob.sentences[1]\n",
    "newblob.sentences[2]\n",
    "\n",
    "\n",
    "\n",
    "wordlist = newblob.tokenize()\n",
    "wordlist\n",
    "\n",
    "lemmas = wordlist.lemmatize()\n",
    "lemmas\n",
    "\n",
    "lemmas.lower()\n",
    "\n",
    "wordlist.stem()\n",
    "\n",
    "newblob.correct()  # this should be called .wrongify()\n",
    "\n",
    "TextBlob(\"Leipzig is a beatifull townn\").correct()  \n",
    "\n",
    "newblob.pos_tags\n",
    "\n",
    "print(\"\\n\".join([word + \"\\t\" + tag for word, tag in newblob.pos_tags]))\n",
    "\n",
    "[word for word, tag in newblob.tags if tag[0] == \"V\"]  \n",
    "\n",
    "\n",
    "print(\"\\n\".join([word + \"\\t\" + tag for word, tag in blob.tags if tag[0] == \"V\"]))\n",
    "\n",
    "\n",
    "newblob.ngrams(3)\n",
    "\n",
    "list(enumerate([\"a\", \"b\", \"c\"]))\n",
    "\n",
    "[ngram for index, ngram in enumerate(newblob.ngrams(3)) if index % 3 == 0]\n",
    "\n",
    "TextBlob(\"Leipzig is a town.\").sentiment\n",
    "\n",
    "TextBlob(\"Leipzig is a beautiful town.\").sentiment\n",
    "\n",
    "TextBlob(\"Leipzig seems to be a beautiful town.\").sentiment\n",
    "\n",
    "TextBlob(\"Leipzig is not a beautiful town.\").sentiment\n",
    "\n",
    "TextBlob(\"Leipzig is an ugly town.\").sentiment\n",
    "\n",
    "TextBlob(\"Leipzig is not an ugly town.\").sentiment  # fail!\n",
    "\n",
    "[sentence.sentiment.polarity for sentence in newblob.sentences]\n",
    "\n",
    "[sentence for sentence in blob.sentences if sentence.polarity > 0.3]\n",
    "\n",
    "choppedblob = TextBlob(text.replace(\";\", \".\").replace(\",\", \".\"))\n",
    "\n",
    "# the magic number for sonnets!\"\"\n",
    "len(choppedblob.sentences)\n",
    "\n",
    "[sentence for sentence in choppedblob.sentences if sentence.polarity > 0.6]\n",
    "\n",
    "[sentence for sentence in choppedblob.sentences if sentence.polarity <= -0.1]\n",
    "\n",
    "import codecs\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "tei_header, tei_text = ElementTree.fromstring(\n",
    "    codecs.open(\"macbeth.xml\", \"r\", \"utf-8\").read()\n",
    ")\n",
    "textnodes_act1 = [textnodes for textnodes in tei_text[1][0].itertext()]\n",
    "text = \" \".join(textnodes_act1).replace(\"\\n\", \" \")\n",
    "len(text)\n",
    "\n",
    "macblob = TextBlob(text)\n",
    "\n",
    "macblob.sentences[0]\n",
    "\n",
    "macblob.sentences[0].tokens\n",
    "\n",
    "\" \".join(macblob.sentences[0].tokens)\n",
    "\n",
    "\n",
    "blobs = [TextBlob(\" \".join(sentence.tokens)) for sentence in macblob.sentences]\n",
    "\n",
    "polarities = [blob.polarity for blob in blobs]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(polarities)\n",
    "plt.show()\n",
    "\n",
    "subjectivities = [blob.subjectivity for blob in blobs]\n",
    "\n",
    "plt.plot(polarities)\n",
    "plt.plot(subjectivities)\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(np.array(polarities) * np.array(subjectivities))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "textnodes = [textnodes for textnodes in tei_text[1].itertext()]\n",
    "text = \" \".join(textnodes).replace(\"\\n\", \" \")\n",
    "macblob = TextBlob(text)\n",
    "blobs = [TextBlob(\" \".join(sentence.tokens)) for sentence in macblob.sentences]\n",
    "polarities = [blob.polarity for blob in blobs]\n",
    "subjectivities = [blob.subjectivity for blob in blobs]\n",
    "series = np.array(polarities) * np.array(subjectivities)\n",
    "plt.plot(series)\n",
    "plt.show()\n",
    "\n",
    "# let's get \"scientific\"!\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "smoothed = savgol_filter(series, 5, 3)\n",
    "plt.plot(series)\n",
    "plt.plot(smoothed)\n",
    "plt.show()\n",
    "\n",
    "# signal or noise?\n",
    "smoothed1 = savgol_filter(series, 25, 3)\n",
    "plt.plot(smoothed1)\n",
    "smoothed2 = savgol_filter(series, 1000, 5)\n",
    "plt.plot(smoothed2)\n",
    "plt.show()\n",
    "\n",
    "smoothed_polarities = savgol_filter(polarities, 25, 3)\n",
    "plt.plot(smoothed_polarities)\n",
    "smoothed_series = savgol_filter(series, 25, 3)\n",
    "plt.plot(smoothed_series)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "yay = [sentence for sentence in blobs if sentence.polarity == 1]\n",
    "nay = [sentence for sentence in blobs if sentence.polarity == -1]\n",
    "\n",
    "yay\n",
    "nay\n",
    "\n",
    "# most words do not get assigned a polarity on their own\n",
    "[(token, TextBlob(token).polarity) for token in yay[0].tokens]\n",
    "[(token, TextBlob(token).polarity) for token in nay[0].tokens]\n",
    "\n",
    "# let's grab all the words from both lists\n",
    "yaywords = list(set((\" \".join([x.string for x in yay])).split()))\n",
    "naywords = list(set((\" \".join([x.string for x in nay])).split()))\n",
    "\n",
    "# only few of the words in the best or worst sentences get a polarity score\n",
    "# when isolated\n",
    "[token for token in yaywords if TextBlob(token).polarity > 0]\n",
    "[token for token in naywords if TextBlob(token).polarity < 0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}